{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4816e7",
   "metadata": {},
   "source": [
    "# CELL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7a7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      " Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from augmentation import AudioAugmentation  # Import augmentation module\n",
    "import gc\n",
    "\n",
    "print(\" Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5345b3",
   "metadata": {},
   "source": [
    "# CELL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f676fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../DATASET_LABELED\" \n",
    "output_dir = \"./whisper_ser_checkpoint_v2\"  # New output dir\n",
    "model_id = \"openai/whisper-base\"\n",
    "\n",
    "# Kiểm tra GPU\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Giải phóng bộ nhớ\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19167c2",
   "metadata": {},
   "source": [
    "# CELL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217b5e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Found 977 audio files with 5 classes\n",
      "Dataset loaded! Classes: ['ANG', 'ANX', 'HAP', 'NEU', 'SAD']\n",
      " Train: 781, Test: 196\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "class AudioDatasetWithAugmentation(Dataset):\n",
    "    \"\"\"Dataset với improved augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, target_sr=16000, augment=False, augment_prob=0.5):\n",
    "        self.data_dir = data_dir\n",
    "        self.target_sr = target_sr\n",
    "        self.augment = augment\n",
    "        self.augment_prob = augment_prob\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.label_names = []\n",
    "        \n",
    "        # Khởi tạo augmentor nếu cần\n",
    "        if self.augment:\n",
    "            self.augmentor = AudioAugmentation(target_sr)\n",
    "        \n",
    "        # Load data\n",
    "        for label_name in sorted(os.listdir(data_dir)):\n",
    "            label_path = os.path.join(data_dir, label_name)\n",
    "            if os.path.isdir(label_path):\n",
    "                if label_name not in self.label_names:\n",
    "                    self.label_names.append(label_name)\n",
    "                label_id = self.label_names.index(label_name)\n",
    "                \n",
    "                for file_name in os.listdir(label_path):\n",
    "                    if file_name.endswith(('.wav', '.mp3', '.flac', '.ogg')):\n",
    "                        self.samples.append(os.path.join(label_path, file_name))\n",
    "                        self.labels.append(label_id)\n",
    "        \n",
    "        print(f\"Found {len(self.samples)} audio files with {len(self.label_names)} classes\")\n",
    "        if self.augment:\n",
    "            print(f\"Data augmentation: ENABLED (prob={augment_prob})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Resample\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # Apply augmentation (improved: speed change, noise, time masking, gain)\n",
    "        if self.augment:\n",
    "            waveform = self.augmentor.augment_batch(waveform.squeeze(0), self.augment_prob)\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"audio\": {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": self.target_sr},\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "full_dataset = AudioDatasetWithAugmentation(data_path, augment=False)\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, test_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Get labels\n",
    "labels = full_dataset.label_names\n",
    "label2id = {label: str(i) for i, label in enumerate(labels)}\n",
    "id2label = {str(i): label for i, label in enumerate(labels)}\n",
    "\n",
    "print(f\"Dataset loaded! Classes: {labels}\")\n",
    "print(f\" Train: {len(train_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ab3ce",
   "metadata": {},
   "source": [
    "# CELL 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52395f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preprocessing configured with IMPROVED augmentation!\n",
      "   - Speed change (resampling)\n",
      "   - Noise injection (energy-scaled, 0.003-0.008)\n",
      "   - Time masking (0-10%)\n",
      "   - Random gain (0.8-1.2x)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "\n",
    "class PreprocessedDatasetWithAugmentation(Dataset):\n",
    "    \"\"\"Preprocessed dataset with improved augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, subset, feature_extractor, max_length=16000*30, augment=False):\n",
    "        self.subset = subset\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Augmentor for training\n",
    "        if self.augment:\n",
    "            self.augmentor = AudioAugmentation(16000)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.subset[idx]\n",
    "        audio_array = item[\"audio\"][\"array\"]\n",
    "        \n",
    "        # Apply augmentation (improved techniques)\n",
    "        if self.augment:\n",
    "            audio_array = self.augmentor.augment_batch(\n",
    "                torch.from_numpy(audio_array), \n",
    "                augment_prob=0.6\n",
    "            ).numpy()\n",
    "        \n",
    "        inputs = self.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": inputs[\"input_features\"].squeeze(0),\n",
    "            \"labels\": item[\"label\"]\n",
    "        }\n",
    "\n",
    "# Training has augmentation, test does NOT\n",
    "encoded_train = PreprocessedDatasetWithAugmentation(train_dataset, feature_extractor, augment=True)\n",
    "encoded_test = PreprocessedDatasetWithAugmentation(test_dataset, feature_extractor, augment=False)\n",
    "\n",
    "print(\" Preprocessing configured with IMPROVED augmentation!\")\n",
    "print(\"   - Speed change (resampling)\")\n",
    "print(\"   - Noise injection (energy-scaled, 0.003-0.008)\")\n",
    "print(\"   - Time masking (0-10%)\")\n",
    "print(\"   - Random gain (0.8-1.2x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66115957",
   "metadata": {},
   "source": [
    "# CELL 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4fcdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of WhisperForAudioClassification were not initialized from the model checkpoint at openai/whisper-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainable params: 6,436,357 / 20,723,205 (31.06%)\n",
      " Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "print(\" Loading model...\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(labels),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# FREEZE encoder (all layers first)\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# UNFREEZE last 2 encoder layers (NEW!)\n",
    "for param in model.encoder.layers[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze projector & classifier\n",
    "for param in model.projector.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\" Trainable params: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "print(f\" Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577a172",
   "metadata": {},
   "source": [
    "# CELL 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22900d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class weights (BOOSTED):\n",
      "  ANG: 1.659 (count: 166)\n",
      "  ANX: 2.562 (count: 93)\n",
      "  HAP: 2.103 (count: 121)\n",
      "  NEU: 1.000 (count: 326)\n",
      "  SAD: 3.010 (count: 75)\n",
      "\n",
      "============================================================\n",
      " TRAINER CONFIGURED - PHASE 1 IMPROVEMENTS\n",
      "============================================================\n",
      "\n",
      " IMPROVED DATA AUGMENTATION:\n",
      "   1. Speed Change: 0.9x - 1.1x (Resampling)\n",
      "   2. Noise: 0.003 - 0.008 (Energy-scaled)\n",
      "   3. Time Masking: 0-10% ⭐ NEW\n",
      "   4. Random Gain: 0.8x - 1.2x\n",
      "\n",
      " LOSS FUNCTION:\n",
      "   - Focal Loss: gamma=2.0\n",
      "   - Class weights: power=0.75\n",
      "\n",
      " HYPERPARAMETERS:\n",
      "   - Learning rate: 3e-4\n",
      "   - Batch size: 8\n",
      "   - Epochs: 50\n",
      "   - Gradient clipping: 1.0 ⭐ NEW\n",
      "\n",
      " TARGET: >60% accuracy, all classes >30% recall\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "# Calculate BOOSTED class weights\n",
    "from collections import Counter\n",
    "train_labels = [full_dataset.labels[i] for i in train_dataset.indices]\n",
    "label_counts = Counter(train_labels)\n",
    "num_classes = len(labels)\n",
    "\n",
    "# Stronger class weights (power=1.0)\n",
    "max_count = max(label_counts.values())\n",
    "class_weights = torch.tensor([\n",
    "    (max_count / label_counts[i]) ** 0.75  # Increased from 0.75\n",
    "    for i in range(num_classes)\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "print(\" Class weights (BOOSTED):\")\n",
    "for i, label_name in enumerate(labels):\n",
    "    print(f\"  {label_name}: {class_weights[i].item():.3f} (count: {label_counts[i]})\")\n",
    "\n",
    "# Enhanced Focal Loss (gamma=3.0)\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(\n",
    "            inputs, targets, weight=self.weight, reduction='none',\n",
    "            label_smoothing=0.1  # NEW!\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "class FocalTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLoss(weight=class_weights, gamma=2.0)  # Changed!\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels_batch = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.focal_loss(logits, labels_batch)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def data_collator(features):\n",
    "    input_features = [f[\"input_features\"] for f in features] \n",
    "    labels_list = [f[\"labels\"] for f in features]\n",
    "    \n",
    "    return {\n",
    "        \"input_features\": torch.tensor(np.array(input_features), dtype=torch.float32),\n",
    "        \"labels\": torch.tensor(np.array(labels_list), dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# IMPROVED TRAINING ARGUMENTS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=3e-4,  # Decreased from 5e-4\n",
    "    per_device_train_batch_size=8,  # Increased from 4\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=50,  # Increased from 30\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False, \n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_num_workers=0,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Use loss instead of accuracy\n",
    "    greater_is_better=False,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    ")\n",
    "\n",
    "trainer = FocalTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" TRAINER CONFIGURED - PHASE 1 IMPROVEMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n IMPROVED DATA AUGMENTATION:\")\n",
    "print(\"   1. Speed Change: 0.9x - 1.1x (Resampling)\")\n",
    "print(\"   2. Noise: 0.003 - 0.008 (Energy-scaled)\")\n",
    "print(\"   3. Time Masking: 0-10% ⭐ NEW\")\n",
    "print(\"   4. Random Gain: 0.8x - 1.2x\")\n",
    "print(\"\\n LOSS FUNCTION:\")\n",
    "print(\"   - Focal Loss: gamma=2.0\")\n",
    "print(\"   - Class weights: power=0.75\")\n",
    "print(\"\\n HYPERPARAMETERS:\")\n",
    "print(\"   - Learning rate: 3e-4\")\n",
    "print(\"   - Batch size: 8\")\n",
    "print(\"   - Epochs: 50\")\n",
    "print(\"   - Gradient clipping: 1.0 ⭐ NEW\")\n",
    "print(\"\\n TARGET: >60% accuracy, all classes >30% recall\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd91a",
   "metadata": {},
   "source": [
    "# CELL 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dfaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " STARTING TRAINING - PHASE 1 QUICK WINS\n",
      "============================================================\n",
      "\n",
      " Training Configuration:\n",
      "   - Total samples: 977\n",
      "   - Train: 781\n",
      "   - Test: 196\n",
      "   - Classes: ['ANG', 'ANX', 'HAP', 'NEU', 'SAD']\n",
      "   - Device: cuda\n",
      "\n",
      " Expected: ~2-3 hours (RTX 3050, 50 epochs)\n",
      " Training auto-saves best model based on eval_loss\n",
      " Can stop early (Interrupt Kernel) if eval_loss plateaus\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4900' max='4900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4900/4900 49:09, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.219300</td>\n",
       "      <td>2.395546</td>\n",
       "      <td>0.270408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.811400</td>\n",
       "      <td>1.966736</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.844200</td>\n",
       "      <td>1.841365</td>\n",
       "      <td>0.566327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.219900</td>\n",
       "      <td>2.033648</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.118600</td>\n",
       "      <td>1.902941</td>\n",
       "      <td>0.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.825900</td>\n",
       "      <td>1.811375</td>\n",
       "      <td>0.622449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.619500</td>\n",
       "      <td>2.161755</td>\n",
       "      <td>0.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>2.277785</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.517700</td>\n",
       "      <td>2.308071</td>\n",
       "      <td>0.622449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>2.643640</td>\n",
       "      <td>0.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>2.413990</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>2.541594</td>\n",
       "      <td>0.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>2.435337</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>2.524905</td>\n",
       "      <td>0.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>2.532396</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>2.543146</td>\n",
       "      <td>0.581633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>2.503602</td>\n",
       "      <td>0.586735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>2.480368</td>\n",
       "      <td>0.617347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>2.272409</td>\n",
       "      <td>0.617347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>2.394501</td>\n",
       "      <td>0.617347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>2.502830</td>\n",
       "      <td>0.622449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>2.610068</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.292300</td>\n",
       "      <td>2.654339</td>\n",
       "      <td>0.586735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>2.677263</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>2.546518</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>2.493424</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>2.634279</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>2.688835</td>\n",
       "      <td>0.581633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>2.801125</td>\n",
       "      <td>0.561224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>2.864547</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>2.814305</td>\n",
       "      <td>0.566327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.272200</td>\n",
       "      <td>2.792261</td>\n",
       "      <td>0.566327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.227600</td>\n",
       "      <td>2.687180</td>\n",
       "      <td>0.566327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.272000</td>\n",
       "      <td>2.653476</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2.656577</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.226500</td>\n",
       "      <td>2.693335</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>2.702673</td>\n",
       "      <td>0.586735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>2.681905</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.709038</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.225500</td>\n",
       "      <td>2.687088</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>2.802824</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>2.715381</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>2.656015</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>2.664619</td>\n",
       "      <td>0.596939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.225200</td>\n",
       "      <td>2.672851</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>2.661928</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>2.663108</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>2.663201</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.224400</td>\n",
       "      <td>2.665483</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>2.665557</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " SAVING FINAL MODEL\n",
      "============================================================\n",
      "\n",
      " Training completed!\n",
      " Model saved to: ./whisper_ser_checkpoint_v2/final_model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\venv\\Lib\\site-packages\\transformers\\configuration_utils.py:461: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" STARTING TRAINING - PHASE 1 QUICK WINS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n Training Configuration:\")\n",
    "print(f\"   - Total samples: {len(full_dataset)}\")\n",
    "print(f\"   - Train: {len(train_dataset)}\")\n",
    "print(f\"   - Test: {len(test_dataset)}\")\n",
    "print(f\"   - Classes: {labels}\")\n",
    "print(f\"   - Device: {device}\")\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# START TRAINING\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" SAVING FINAL MODEL\")\n",
    "print(\"=\"*60)\n",
    "trainer.save_model(output_dir + \"/final_model\")\n",
    "\n",
    "print(\"\\n Training completed!\")\n",
    "print(f\" Model saved to: {output_dir}/final_model\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d7045",
   "metadata": {},
   "source": [
    "# CELL 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bf7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluating model on test set...\n",
      "\n",
      "============================================================\n",
      " CLASSIFICATION REPORT\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ANG       0.65      0.84      0.74        38\n",
      "         ANX       0.59      0.32      0.42        31\n",
      "         HAP       0.41      0.59      0.48        27\n",
      "         NEU       0.71      0.57      0.63        74\n",
      "         SAD       0.69      0.85      0.76        26\n",
      "\n",
      "    accuracy                           0.62       196\n",
      "   macro avg       0.61      0.63      0.61       196\n",
      "weighted avg       0.64      0.62      0.61       196\n",
      "\n",
      "\n",
      "============================================================\n",
      " CONFUSION MATRIX\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PER-CLASS ACCURACY\n",
      "============================================================\n",
      "  ✓ ANG: 84.21%\n",
      "  ✓ ANX: 32.26%\n",
      "  ✓ HAP: 59.26%\n",
      "  ✓ NEU: 56.76%\n",
      "  ✓ SAD: 84.62%\n",
      "\n",
      " OVERALL ACCURACY: 62.24%\n",
      " SUCCESS! Target >60% achieved!\n",
      "\n",
      " Target: >60% accuracy, all classes >30% recall\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\" Evaluating model on test set...\")\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(encoded_test)):\n",
    "        item = encoded_test[i]\n",
    "        input_features = torch.tensor(item[\"input_features\"]).unsqueeze(0).to(device)\n",
    "        label = item[\"labels\"]\n",
    "        \n",
    "        outputs = model(input_features=input_features)\n",
    "        pred = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        \n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(label)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(all_labels, all_preds, target_names=labels, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - SER (Phase 1 Improvements)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "for i, label_name in enumerate(labels):\n",
    "    class_mask = [l == i for l in all_labels]\n",
    "    class_preds = [p for p, m in zip(all_preds, class_mask) if m]\n",
    "    class_labels = [l for l, m in zip(all_labels, class_mask) if m]\n",
    "    if len(class_labels) > 0:\n",
    "        acc = sum(p == l for p, l in zip(class_preds, class_labels)) / len(class_labels)\n",
    "        status = \"✓\" if acc > 0.3 else \"✗\"\n",
    "        print(f\"  {status} {label_name}: {acc*100:.2f}%\")\n",
    "\n",
    "# Overall\n",
    "overall_acc = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "print(f\"\\n OVERALL ACCURACY: {overall_acc*100:.2f}%\")\n",
    "\n",
    "if overall_acc >= 0.6:\n",
    "    print(\" SUCCESS! Target >60% achieved!\")\n",
    "else:\n",
    "    print(f\" Current: {overall_acc*100:.1f}% (Target: >60%)\")\n",
    "\n",
    "print(\"\\n Target: >60% accuracy, all classes >30% recall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
